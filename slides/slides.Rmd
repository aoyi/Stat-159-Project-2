---
title: 'Stat159 Project 2: Predictive Modeling Process'
author: "Aoyi Shan, Yukun He"
date: "11/1/2016"
output: 
  ioslides_presentation:
    incremental: true
---

## Introduction

- In this project, we explore the difference between various muitiple regression methods and conclude on what approach should be used in order to fit our data better 
- In our analysis, we compare the fit of models generated by 5 different regression methods
     - Ordinary Least Squares
     - Ridge regression
     - Lasso regression
     - Principal Components regression
     - Partial Least Squares regression
- We perform 10-fold cross validation in the model construction process and select the model with minimum cross-validation errors in terms of the running parameter

## Data

- The data we used in this project is provided by the textbook _**An Introduction to Statistical Learning**_
- The data set can be downloaded from <http://www-bcf.usc.edu/~gareth/ISL/Credit.csv>
- Pre-modeling Data Processing:
     - Dummy out categorical variables
     - Mean centering and standardizing
- Divide the data into training set and test set:
     - Randomly select 300 entries to be the train set
     - The remaining 100 entries is the test set
     
## Methods | Least Squares Method

- OLS coefficients are set as benchmark to compare the coefficients of the other four regression methods
- OLS estimators are obtained by minimizing the sum of squares $RSS = \sum_{i=1}^n(y_i - \beta_0 - \sum_{j=1}^p(\beta_{j}x_{ij}))^2$ where $\beta_0, ..., \beta_p$ are our coefficient estimates
- OLS is the most common regression method, and it works the best when we have homoskedastic and uncorrelated errors within the data

## Methods | Ridge Regression 

- Instead of minimizing RSS, Ridge regression minimizes $RSS + \lambda \sum bj^2$
- $\lambda$ is called a tuning parameter and it controls how much RSS and shrinkage panelty affects the regression model
- One of the main feature of ridge regression is the bias-variance trade-off

## Methods | Lasso Regression

- The major difference between Lasso and Ridge is that Lasso method aims to minimize $RSS$ + $\lambda \sum |{\beta_j}|$
- Ridge regression will always generate a model with all predictors since it doesn't involve a process of variable reduction
- Lasso improves by allowing some of the coefficient estimates to be exactly 0 if the tunning parameter is sufficiently large
- In general, lasso is expected to outperform ridge when we have a small amount of predictors having significant coefficients, and the rest close to 0

## Methods | Principal Components Regression

- The key assumption we hold in this regression model is that the directions in which $X_1, ..., X_p$ show the most variation are the directions that are associated with Y
- PCR performs the best when the first few pricipal components are sufficient to capture most of the variation in the predictors and their relationship with the response

## Methods | Partial Least Squares Regression

- In comparison to PCR regression, which uses unsupervised method to dentify the principal components, partial least squares method takes the advantage of a supervised learning process
- It uses the response variable Y to identify new vectors that are not only similar to existing regressors, but also select the ones that are related to the response
- PLS approach attempts to find directions that help explain both the response and the predictors

## Model Building Process

- Use 10-fold cross validation in building each regression model
- Use the train set to fit the model
- Calculate MSE in the test set 
- Fit the model we select to the entire data set
     
## Results | OLS Regression Coefficient Estimates 

```{r results= 'asis', echo = FALSE}
library(xtable)
library(Matrix)
options(xtable.comment = FALSE,
        xtable.table.placement = "H")
load("../data/regression-data/ols-model-stats.RData")
load("../data/regression-data/ols-regression.RData")
print(xtable(ols.summary$coefficients, caption = 'OLS Coefficients',digits = c(0,5,5,5,5)), comment = FALSE, type = "html")
```

## Results | OLS Regression Analysis

- Some coefficients come with a big p-value, which means that they are not statistically significant
- We can conclude that the constant term, Education, Gender, Marital Status and Ethinicity don't belong to this regression
- Among the statistically significant regressors, some has very small coefficients, so the main factors influencing Balance are Income, Limit and Rating

## Results | Ridge Regression Coefficient Estimates 

```{r results= 'asis', echo = FALSE}
load("../data/regression-data/ridge-model-stats.RData")
load("../data/regression-data/ridge-regression.RData")
ridge_coeff <- as.matrix(ridge.fitted.coef)
colnames(ridge_coeff) <- "Estimate"
print(xtable(ridge_coeff, caption = 'Ridge Coefficients',digits = c(0,5)), comment = FALSE, type = 'html')
```

## Results | Ridge Regression Analysis

- With Ridge regression, the $\lambda$ we found that results in the smallest validation error is $\lambda$ = `r best.lambda.ridge`
- As we have a relatively small $\lambda$, we expect to see that the estimation with ridge is very similar to that of OLS but a little bit smaller due to the shrinkage effect 

## Results | Lasso Regression Coefficient Estimates 

```{r results= 'asis', echo = FALSE}
load("../data/regression-data/lasso-model-stats.RData")
load("../data/regression-data/lasso-regression.RData")
lasso_coeff <- as.matrix(lasso.fitted.coef)
colnames(lasso_coeff) <- "Estimate"
print(xtable(lasso_coeff, caption = 'Lasso Coefficients', digits = c(0,5)), comment = FALSE,type = 'html')
```

## Results | Lasso Regression Analysis

- The $\lambda$ we found that results in the smallest validation error is $\lambda$ = `r best.lambda.lasso`
- We can see that a significant improvement is that we have a number of regressors with a coefficient of 0 which makes the intepretation much easier

## Results | PCR Coefficient Estimates  

```{r results= 'asis', echo = FALSE}
load("../data/regression-data/pcr-model-stats.RData")
load("../data/regression-data/pcr-regression.RData")
pcr_coeff <- as.matrix(pcr.fitted.coef)
dimnames(pcr_coeff) <- list(rownames(pcr_coeff, do.NULL = FALSE, prefix = "row"),
                            colnames(pcr_coeff, do.NULL = FALSE, prefix = "col"))
colnames(pcr_coeff) <- "Estimate"
rownames(pcr_coeff) <- c("Income", "Limit", "Rating", "Cards", "Age", "Education", "GenderFemale", "StudentYes", "MarriedYes", "EthnicityAsian", "EthnicityCaucasian")
print(xtable(pcr_coeff, caption = 'PCR Coefficients',digits = c(0,5)), comment = FALSE, type= 'html')
```

## Results | PCR Analysis

- With PCR, we mainly focus on dimension reduction by unsupervised learning. 
- By comparing validation errors for different Ms, we decide that the best M to use here is `r best.m.pcr`
- Since we only have 11 predictors in this multiple regression, it is not a huge change, so that explains why PCR is very close to OLS
- The dimension of predictors for this regression almost cannot be reduced, there doesn't exist major principal components dominating the change in the response variable
- PCR doesn't help to improve that much on the OLS estimation 

## Results | PLSR Coefficient Estimates 

```{r results= 'asis', echo = FALSE}
load("../data/regression-data/plsr-model-stats.RData")
load("../data/regression-data/plsr-regression.RData")
plsr_coeff <- as.matrix(plsr.fitted.coef)
dimnames(plsr_coeff) <- list(rownames(plsr_coeff, do.NULL = FALSE, prefix = "row"),
                             colnames(plsr_coeff, do.NULL = FALSE, prefix = "col"))
colnames(plsr_coeff) <- "Estimate"
rownames(plsr_coeff) <- c("Income", "Limit", "Rating", "Cards", "Age", "Education", "GenderFemale", "StudentYes", "MarriedYes", "EthnicityAsian", "EthnicityCaucasian")
print(xtable(plsr_coeff, caption = 'PLS Coefficients',digits = c(0,5)), comment = FALSE, type = "html")
```

## Results | PLSR Analysis 

- Similarly, PLS regression is also trying to reduce the dimension of predictors, but in an supervised way
- By comparing validation errors for different Ms, we decide that the best M to use here is `r best.m.plsr`
- This method successfully find the major principal components dominating the change in Y, and therefore reduce the risk of overfitting and therefore obtain a better fit

## Comparing the Coefficient Estimates for 5 Regression Models 

```{r results= 'asis', echo = FALSE}
reg.coef.mat = cbind(ols.fitted.coef, as.numeric(ridge.fitted.coef), as.numeric(lasso.fitted.coef), c(0,pcr.fitted.coef), c(0,plsr.fitted.coef))
colnames(reg.coef.mat) = c("ols", "ridge", "lasso", "pcr", "plsr")
print(xtable(reg.coef.mat, caption = 'Regression Coefficients for 5 Regression Methods', digits = c(0,5,5,5,5,5)), comment = FALSE, type = "html")
```

## Comparing the MSE of 5 Regression Models 

```{r results= 'asis', echo = FALSE}
source("../code/functions/regression-functions.R")
full.set = read.csv("../data/data-sets/scaled-credit.csv", header = T)
MSE.val = as.matrix(c("ols" = MSE.ols, "ridge" = MSE.ridge, "lasso" = MSE.lasso, "pcr" = MSE.pcr, "plsr" = MSE.plsr))
colnames(MSE.val) = "MSE"
print(xtable(MSE.val, caption = 'MSE and RSS of 5 Regression Methods', digits = 5), comment = FALSE, type = "html")
```

## Trend Lines of Coefficient Estimates of 5 Regression Models 

```{r results= 'asis', echo = FALSE}
par(mfrow = c(1,1))
plot(reg.coef.mat[,1], xlab = "Coefficients", ylab = "Value", main = "Trend Lines of Coefficients for Different Regression Models")
lines(reg.coef.mat[,1])
points(reg.coef.mat[,2],col= "blue")
lines(reg.coef.mat[,2],col = "blue")
points(reg.coef.mat[,3], col = "red")
lines(reg.coef.mat[,3],col = "red")
points(reg.coef.mat[,4], col = "green")
lines(reg.coef.mat[,4],col = "green")
points(reg.coef.mat[,5], col = "yellow")
lines(reg.coef.mat[,5],col = "yellow")
legend(10,0.9,c('OLS','Ridge','Lasso','PCR','PLSR'), lty = c(1,1,1,1,1), lwd = c(2.5,2.5,2.5,2.5,2.5), col = c("black","blue","red","green","yellow"),merge = T)
abline(h = 0, lty = 3)
```

## Conclusion

- Combining the coefficients from all five regression models, we notice that only the coefficients for Limit, Rating and StudentYes varies and a number of regressors have coefficients close to 0
- Each model has its own feature and can lead to the optimal regression model under different circumstances
- For the data set we work on in this project, both our analysis and MSE shows that Lasso Regression model fits the best, followed by OLS and PlS, and appropriate models lead to more accurate prediction
