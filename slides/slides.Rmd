---
title: 'Stat159 Project 2: Predictive Modeling Process'
author: "Aoyi Shan, Yukun He"
date: "11/1/2016"
output: 
    ioslides_presentation:
       incremental: true
---

## Introduction

- In this project, we explore the difference between various muitiple regression methods and conclude on what approach should be used in order to fit our data better 
- In our analysis, we compare the fit of models generated by 5 different regression methods
     - Ordinary Least Squares
     - Ridge regression
     - Lasso regression
     - Principal Components regression
     - Partial Least Squares regression
- We perform 10-fold cross validation in the model construction process and select the model with minimum cross-validation errors in terms of the running parameter

## Data

- The data we used in this project is provided by the textbook _**An Introduction to Statistical Learning**_
- The data set can be downloaded from <http://www-bcf.usc.edu/~gareth/ISL/Credit.csv>
- Pre-modeling Data Processing:
     - Dummy out categorical variables
     - Mean centering and standardizing
- Divide the data into training set and test set:
     - Randomly select 300 entries to be the train set
     - The remaining 100 entries is the test set
     
## Methods | Least Squares Method

- OLS coefficients are set as benchmark to compare the coefficients of the other four regression methods
- OLS estimators are obtained by minimizing the sum of squares $RSS = \sum_{i=1}^n(y_i - \beta_0 - \sum_{j=1}^p(\beta_{j}x_{ij}))^2$ where $\beta_0, ..., \beta_p$ are our coefficient estimates
- OLS is the most common regression method, and it works the best when we have homoskedastic and uncorrelated errors within the data

## Methods | Ridge Regression 

- Instead of minimizing RSS, Ridge regression minimizes $RSS + \lambda \sum bj^2$
- $\lambda$ is called a tuning parameter and it controls how much RSS and shrinkage panelty affects the regression model
- One of the main feature of ridge regression is the bias-variance trade-off

## Methods | Lasso Regression

- The major difference between Lasso and Ridge is that Lasso method aims to minimize $RSS$ + $\lambda \sum |{\beta_j}|$
- Ridge regression will always generate a model with all predictors since it doesn't involve a process of variable reduction
- Lasso improves by allowing some of the coefficient estimates to be exactly 0 if the tunning parameter is sufficiently large
- In general, lasso is expected to outperform ridge when we have a small amount of predictors having significant coefficients, and the rest close to 0

## Methods | Principal Components Regression

- The key assumption we hold in this regression model is that the directions in which $X_1, ..., X_p$ show the most variation are the directions that are associated with Y
- PCR performs the best when the first few pricipal components are sufficient to capture most of the variation in the predictors and their relationship with the response

## Methods | Partial Least Squares Regression

- In comparison to PCR regression, which uses unsupervised method to dentify the principal components, partial least squares method takes the advantage of a supervised learning process
- It uses the response variable Y to identify new vectors that are not only similar to existing regressors, but also select the ones that are related to the response
- PLS approach attempts to find directions that help explain both the response and the predictors
     
## Results | OLS Regression Coefficient Estimates {.smaller .flexbox .vcenter}
```{r results= 'asis', echo = FALSE}
library(knitr)
library(xtable)
library(Matrix)
load("../data/regression-data/ols-model-stats.RData")
load("../data/regression-data/ols-regression.RData")
kable(ols.summary$coefficients, caption = 'OLS Coefficients',digits = c(5,5,5,5))
```

## Results | OLS Regression Results

Since OLS would be the base case in this project, we also use 10-fold cross validation in building the OLS model by first using the train set to fit the model, calculate MSE in the test set and then fit the model we select to the entire data set. From the OLS regression output, we noticed some coefficients come with a big p-value, which means that they are not statistically significant. Therefore, we can conclude that the constant term, Education, Gender, Marital Status and Ethinicity don't belong to this regression. Also we noticed that among the statistically significant regressors, some has very small coefficients, so the main factors influencing Balance are Income, Limit and Rating.

## Ridge Regression 

```{r xtable, results= 'asis', echo = FALSE}
load("../data/regression-data/ridge-model-stats.RData")
load("../data/regression-data/ridge-regression.RData")
ridge_coeff <- as.matrix(ridge.fitted.coef)
colnames(ridge_coeff) <- "Estimate"
print(xtable(ridge_coeff, caption = 'Ridge Coefficients',digits = c(0,5)), comment = FALSE, type = 'latex')
```

With Ridge regression, the $\lambda$ we found that results in the smallest validation error is $\lambda$ = `r best.lambda.ridge`. As we have a relatively small $\lambda$, we expect to see that the estimation with ridge is very similar to that of OLS but a little bit smaller due to the shrinkage effect. 

## Lasso Regression {.smaller}

```{r, results= 'asis', echo = FALSE}
library(knitr)
load("../data/regression-data/lasso-model-stats.RData")
load("../data/regression-data/lasso-regression.RData")
lasso_coeff <- as.matrix(lasso.fitted.coef)
colnames(lasso_coeff) <- "Estimate"
kable(lasso_coeff, caption = 'Lasso Coefficients', digits = 5)
```

Lasso improves on Ridge because it adds the incentive to render statistically insignificant estimates to 0 by performing both variable selection and yields model that involves only a subset of the variables. In this case, the $\lambda$ we found that results in the smallest validation error is $\lambda$ = `r best.lambda.lasso`. We can see that a significant improvement is that we have a number of regressors with a coefficient of 0 which makes the intepretation much easier. 

## PCR Regression 

```{r results= 'asis', echo = FALSE}
load("../data/regression-data/pcr-model-stats.RData")
load("../data/regression-data/pcr-regression.RData")
pcr_coeff <- as.matrix(pcr.fitted.coef)
dimnames(pcr_coeff) <- list(rownames(pcr_coeff, do.NULL = FALSE, prefix = "row"),
                            colnames(pcr_coeff, do.NULL = FALSE, prefix = "col"))
colnames(pcr_coeff) <- "Estimate"
rownames(pcr_coeff) <- c("Income", "Limit", "Rating", "Cards", "Age", "Education", "GenderFemale", "StudentYes", "MarriedYes", "EthnicityAsian", "EthnicityCaucasian")
print(xtable(pcr_coeff, caption = 'PCR Coefficients',digits = c(0,5)), comment = FALSE)
```

With PCR, we mainly focus on dimension reduction by unsupervised learning. By comparing validation errors for different Ms, we decide that the best M to use here is `r best.m.pcr`. Since we only have 11 predictors in this multiple regression, it is not a huge change, so that explains why PCR is very close to OLS. Therefore, we can say that the dimension of predictors for this regression almost cannot be reduced, there doesn't exist major principal components dominating the change in the response variable, so PCR doesn't help to improve that much on the OLS estimation. 

## PLS Regression 

```{r results= 'asis', echo = FALSE}
load("../data/regression-data/plsr-model-stats.RData")
load("../data/regression-data/plsr-regression.RData")
plsr_coeff <- as.matrix(plsr.fitted.coef)
dimnames(plsr_coeff) <- list(rownames(plsr_coeff, do.NULL = FALSE, prefix = "row"),
                             colnames(plsr_coeff, do.NULL = FALSE, prefix = "col"))
colnames(plsr_coeff) <- "Estimate"
rownames(plsr_coeff) <- c("Income", "Limit", "Rating", "Cards", "Age", "Education", "GenderFemale", "StudentYes", "MarriedYes", "EthnicityAsian", "EthnicityCaucasian")
print(xtable(plsr_coeff, caption = 'PLS Coefficients',digits = c(0,5)), comment = FALSE)
```

Similarly, PLS regression is also trying to reduce the dimension of predictors, but in an supervised way. By comparing validation errors for different Ms, we decide that the best M to use here is `r best.m.plsr`. Since we originally have 11 predictors in this multiple regression, this is a huge improvement, so this method successfully find the major principal components dominating the change in Y, and therefore reduce the risk of overfitting and therefore obtain a better fit. 

## Combined All Coefficients Together
```{r kable, results= 'asis', echo = FALSE}
library(knitr)
reg.coef.mat = cbind(ols.fitted.coef, as.numeric(ridge.fitted.coef), as.numeric(lasso.fitted.coef), c(0,pcr.fitted.coef), c(0,plsr.fitted.coef))
colnames(reg.coef.mat) = c("ols", "ridge", "lasso", "pcr", "plsr")
kable(reg.coef.mat, caption = 'Regression Coefficients for 5 Regression Methods', digits = c(5,5,5,5,5),type = 'html')
```




```{r results= 'asis', echo = FALSE}
source("../code/functions/regression-functions.R")
full.set = read.csv("../data/data-sets/scaled-credit.csv", header = T)
MSE.val = as.matrix(c("ols" = MSE.ols, "ridge" = MSE.ridge, "lasso" = MSE.lasso, "pcr" = MSE.pcr, "plsr" = MSE.plsr))
colnames(MSE.val) = "MSE"
print(xtable(MSE.val, caption = 'MSE and RSS of 5 Regression Methods', digits = 5), comment = FALSE)
```


```{r results= 'asis', echo = FALSE}
par(mfrow = c(1,1))
plot(reg.coef.mat[,1], xlab = "Coefficients", ylab = "Value", main = "Trend Lines of Coefficients for Different Regression Models")
lines(reg.coef.mat[,1])
points(reg.coef.mat[,2],col= "blue")
lines(reg.coef.mat[,2],col = "blue")
points(reg.coef.mat[,3], col = "red")
lines(reg.coef.mat[,3],col = "red")
points(reg.coef.mat[,4], col = "green")
lines(reg.coef.mat[,4],col = "green")
points(reg.coef.mat[,5], col = "yellow")
lines(reg.coef.mat[,5],col = "yellow")
legend(10,0.9,c('OLS','Ridge','Lasso','PCR','PLSR'), lty = c(1,1,1,1,1), lwd = c(2.5,2.5,2.5,2.5,2.5), col = c("black","blue","red","green","yellow"),merge = T)
abline(h = 0, lty = 3)
```

## Conclusion

Combining the coefficients from all five regression models, we notice that only the coefficients for Limit, Rating and StudentYes varies and a number of regressors have coefficients close to 0. As we explained above, each model has its own feature and can lead to the optimal regression model under different circumstances. Here, for the data set we work on in this project, both our analysis and MSE shows that Lasso Regression model fits the best, followed by OLS and PlS, and appropriate models lead to more accurate prediction.  
